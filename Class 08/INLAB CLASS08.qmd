---
title: "Class 08 Mini-Project"
author: "Jesus (A17597539)"
format: pdf
---
##Outline 
Today we will apply the machine learning methods we introduce the last class on breast cancer biospy data from fine needle

## Data Input
The data is supplied on CSV format:
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)

```

Now I will store the diagnosis column for later and exclude it from the data set I will actualy do things with that I will call `wisc.data`
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
wisc.data
```

```{r}
# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis
```

>Q1. There are 569 observations in the dataset

```{r}
dim(wisc.data)
```

>Q2. There are 212 malignant observations in the dataset

```{r}
table(wisc.df$diagnosis)
sum(wisc.df$diagnosis == "M")

```

>Q3. There are 10 amount of variables are suffixed with mean

```{r}
colnames(wisc.df)
length(grep("_mean", names(wisc.df)))
```


#Principal Componenet Analysis

We need to scale our inout data before PCA as some of the columns are measured in terms of very different units with different means and different variances. The upshot here is we set `scale=T` argument to `prcomp()`

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=T )

```

```{r}
# Look at summary of results
summary(wisc.pr)
```

Generate one of our main result figures - the PC plot(a.k.a "score plot, "orientation plot", "PC1 vs PC2 plot, "PC plot", "projection plot", etc.) It is known by different names in different fields
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, pch=16)
```

>Q4 From your results, what proportion of the original variance is captured by the first principal components (PC1)? 

0.4427

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

PC1, PC2, and PC3


>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

PC1-PC7


>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

What this plot stands out to me is how dense the points are and how hard it is to read the plot. It is difficult to read the plot because all the labels are overlapping and can not differentiate one point to another

```{r}
biplot(wisc.pr)
```


```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

I notice that the values of PC3 are less than that of PC2 and the spread of the malignant diagnosis is roughly similar

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

For PC1 the component of the loading vectore for the feature concave.points_mean is -0.26085376

```{r}
wisc.pr$rotation[, 1]

```


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimal number of PCc required to explain 80% of variance is PC1-PC5
```{r}
summary(wisc.pr)
```


```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)

```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Having 8 clusters allows us to have cluster number two to be more specfic with having only 79 malignant diagnoses. This can help us be more specfic with our interpretation of the data
```{r}
wisc.hclust.clusters1 <- cutree(wisc.hclust, k=8)

table(wisc.hclust.clusters1, diagnosis)
```

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

My favorite method to get my results for same data.dist dataset is ward.d2 because the clusters have the most minimal amount of variance.



##5 Combining Methods

This approach will take not original data but our PCA results and work with them

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

This new model helps us differentiate the two diagnoses within each cluster. This model is much more specific. We can see that the first cluster has more malignant diagnoses compared to the second cluster

```{r}
d <- dist(wisc.pr$x[, 1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
grps
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```
```{r}
table(grps)
```

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```
