---
title: "Class 07: Machine Learning 1"
author: "Jesus (A17597539)"
format: pdf
---

#Clustering

We will start today's lab eith clustering methods, in particular so-called K-means. The main function for this in R is `kmeans()`. 

Let's try it on some made up data where we know what the answer should be.

```{r}
x <- rnorm(10000, mean=3)
hist(x)
```

60 points
```{r}
tmp <- c(rnorm(30, mean=3), rnorm(30, mean= -3))
x <- cbind(x=tmp, y=rev(tmp))
head(x)

```

We can pass this to the base R `plot()` function for a quick
```{r}
plot(x)
```

```{r}
k <- kmeans(x, centers=2, nstart=20)
k
```

>Q1. How many points are in each cluster?

```{r}
k$size
```

>Q2. Cluster membership?

```{r}
k$cluster

```

>Q3. Cluster Centers?

```{r}
k$centers
```

>Q4. Plot my clustering results

```{r}
plot(x, col=k$cluster, pch=16)
```

>Q5. Cluster the data again with kmeans() into 4 groups and plot the results.

```{r}
k4 <- kmeans(x, centers=4, nstart=20)
plot(x, col=k4$cluster, pch=16)
```

K-mean is very popular mostly because it is fast and relatively straight forward to run and understand. It has a big limitation in that you needs to tell it how many grouos (k, or centers) you want.


#Hierarchical clustering

The main function in base R is called `hclust()`. You have to pass it in a "distance matrix" not just your input data

You can generate a distance matix with the `dist()` funciton.

```{r}
hc <- hclust( dist(x) )
hc
```

```{r}
plot(hc)
```

To find the clusters (cluster memebership vector) from a `hclust()` result we can "cut" the tree at a certain height that we like. For this we use the `

```{r}
plot(hc)
abline(h=8, col="red")
grps <- cutree(hc, h=8)
```

```{r}
table(grps)
```

>Q6. Plot our hclust results

```{r}
plot(x, col=grps, pch=16)
```
#Principal Componenet Analysis


##PCA of UK food data

Read data showing the consumption in grams (per person, per week) of 17 different types of food-stuff measured and averaged in the four countries of the United Kingdom in 1997

 Let's see how PCA can help us but first we can try conventional analysis
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

Q1. 5 columns and 17 rows
```{r}
nrow(x)
ncol(x)
```

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

Second approach to correct row names
```{r}
x <- read.csv(url, row.names=1)
head(x)
```

Q2. I prefer to use the second approach to correc tour row-names problem because it is faster and takes less work for me. The second approach is more robust because if you rerun the first approach it will overwrite itself.


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
 Q3. 
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

A pairs plot can be useful if we dont have 2 dimensions...
```{r}
pairs(x, col=rainbow(17), pch=16, cex=2)
```

Q5. This pairwise plot shows a matrix of scatterplots. Each country has its own respective row and column. The rainbow code give each of the 17 food types its own color. If a point lies within the diagonal line for a given plot, it means that the food type is consumes equally in those countries

Q6. The main difference we can see between N. Irelandis that they have different amount of consumption of food types and this can be seen as the best of fit line is not very diagonal compared to the others. 


###Principal Componenet Analysis

PCA can help us make sense of these types of datasets. Let's see how it works.

The main function in "base" R is called `prcomp()`. In this case we want to first take the transpose `t()` of our input `x` so the columns are the food types and the countries are the rows

```{r}
head( t(x) )
```

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

Q7.

```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2], col= c("orange", "red", "blue", "darkgreen"), pch=16 )
```

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

Q8
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col= c("orange", "red", "blue", "darkgreen"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


The loadings tell us how much of the originalvariables (in our case the foods) contribute to the new variables i.e. the PCs

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance ]
head(pca$rotation)
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
head(pca$rotation)
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
Q9. The two prominent good groups are fresh potatoes and soft drinks. PC2 mainly tells us about the more prominent food groups because of the lesser amount of variance with PC2 
